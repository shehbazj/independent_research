# CSC494 - Assignment 3

## Application Write Granularity

### application writing 1 byte to disk

For each file system in default mode (ext4 ordered, btrfs - ssd and f2fs )
- Format the workload disk and create a new file system. mount the file system at /mnt
- create a file /mnt/abcd
- Enable blocktrace on your workload device.
- Use blocktrace and report amount of write done by file system on disk, when 1 byte is written by your program on /mnt/abcd. you can use dd command with bs=1 and count=1. Take the input device as /dev/urandom instead of /dev/zero.
- Clear the buffer cache once the write is done. this can be done by the following command:
```
	sudo su
	echo 3 > /proc/sys/vm/drop_caches
	exit
```
- read the file using cat /dev/abcd. note how much read is done at file system level for each file system when application sends 1 byte read request. report your observations.
	
### Large file

Note: this experiment should be done for only BTRFS file system in both modes - SSD and HDD.
Format and mount your workload disk with btrfs SSD mode.
Create a 1GB file using dd. Vary the block sizes ( 1024, 4096, 16K, 256K, 1MB) and count accordingly so that total file size is 1GB. Check what granularity writes happen on the disk in SSD mode. Recall from a2, the size of the writes is recorded at sector granularity.
Report the total writes done, mean, maximum and minimum size of writes by dd.

## dm-trace

In this assignment, you will be running a customized device mapper module. The module is already being given to you so you would (hopefully) not have to make any modifications. the module can be accessed [here](https://github.com/shehbazj/independent_research/tree/master/hw3/dm-trace): 
It consists of the following four files:
- Makefile
- dm-trace.c - a trace file that intercepts all reads and writes between the file system and the disk.
- starter.sh - a shell script that would format your file system, create a device mapper, mount the device mapper.
- cleanup.sh - a shell script that would remove your file system.

NOTE: the script/code has been minimally tested with ext4, please report any issues that you may face to the TA while setting up the device mapper tool.

- Copy all four files in one folder.
- run `make`. Let us know if things dont compile.
- run ./setup.sh and provide the workload device name, file system type and device size(in GB). let us know if things don't work.
- run a simple dd command, write 4KB x 10 blocks, see if the writes you observe correspond to your understanding of the file system. run this for each of the 3 file systems.

- Run fio for upto 10 mins. Fill the 20 GB workload disk. Collect the output of the dm-trace tool.
- Re - run your blktrace for 10 mins with the same setup configuration. Collect the output of the dm-trace tool.
- Re - run fio again for 10 mins, again with the same setup. this time do not use any trace tool.
- Draw a graph for each of the 3 file systems comparing the overheads of blktrace v/s dm-trace.

NOTE: Again, if you see dm-trace crashes or setup.sh script does not work, please email the TA. He (usually) knows things.

## Applications

We now run custom applications. This is group specific. Your task is to study and tell us what these applications do and how these applications run. Your task is also to understand various benchmarks and write instructions to setup, install and run each of these applications and corresponding benchmarks on a Ubuntu Machine.

applications and workloads

Group Specific:

### TPCC+ and mysql

Group : Bryan and KC

Read about TPC+ and mysql.<br/>
https://github.com/oltpbenchmark/oltpbench<br/>
https://github.com/Percona-Lab/tpcc-mysql<br/>
https://www.percona.com/blog/2010/02/08/introducing-tpce-like-workload-for-mysql/<br/>


Install and run OLTPBench with MySQL on the VM.
Document Instructions to install MySQL.

### Spark + SparkBench

Group : Yuhan and Fukka

Install Spark on your VM. Install different workloads that exist for the application. <br/>
Document installation and run instructions.<br/>
Talk about this in class for a few minutes. <br/>
https://codait.github.io/spark-bench/workloads/<br/>

### YCSB + Cassandra/RocksDB

Group : Jialun and Tony

Install and run YCSB with Cassandra and/or RocksDB. YCSB contains 6 (or more?) workloads.<br/>
Read about Cassandra and/or RocksDB, install and run YCSB and document setup instructions.<br/>

https://github.com/brianfrankcooper/YCSB/tree/master/cassandra<br/>
https://github.com/brianfrankcooper/YCSB/wiki/Core-Workloads<br/>
https://github.com/brianfrankcooper/YCSB/wiki/Running-a-Workload<br/>
